{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Lenguaje Natural con spaCy\n",
    "## An álisis: El Corazón Delator\n",
    "Análisis completo usando spaCy en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"El_corazón_delator.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texto = f.read()\n",
    "print(f\"Longitud: {len(texto)} caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texto)\n",
    "print(f\"Total tokens: {len(doc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(f\"Total oraciones: {len(sentences)}\")\n",
    "for i, s in enumerate(sentences[:3], 1):\n",
    "    print(f\"{i}. {s.text[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tercera = sentences[2] if len(sentences) >= 3 else None\n",
    "print(\"\\nTercera oración:\")\n",
    "if tercera:\n",
    "    print(f\"{tercera.text}\")\n",
    "    print(\"\\nTokens de la tercera oración:\")\n",
    "    print(f\"{\\\"Token\\\":<15} {\\\"POS\\\":<10} {\\\"DEP\\\":<12} {\\\"LEMMA\\\":<15}\")\n",
    "    for token in tercera:\n",
    "        print(f\"{token.text:<15} {token.pos_:<10} {token.dep_:<12} {token.lemma_:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(f\"\\nSintagmas nominales: {len(noun_chunks)}\")\n",
    "print(\"\\nPrimeros 15 sintagmas nominales:\")\n",
    "for i, chunk in enumerate(noun_chunks[:15], 1):\n",
    "    print(f\"{i}. {chunk.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbos = [t for t in doc if t.pos_ == \"VERB\"]\n",
    "print(f\"\\nVerbos totales: {len(verbos)}\")\n",
    "verbos_unicos = sorted(set(t.lemma_ for t in verbos))\n",
    "print(f\"Verbos únicos: {len(verbos_unicos)}\")\n",
    "print(\"\\nVerbos encontrados:\")\n",
    "for i, verbo in enumerate(verbos_unicos[:20], 1):\n",
    "    print(f\"{i}. {verbo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEntidades Nombradas:\")\n",
    "entidades_dict = {}\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ not in entidades_dict:\n",
    "        entidades_dict[ent.label_] = []\n",
    "    if ent.text not in entidades_dict[ent.label_]:\n",
    "        entidades_dict[ent.label_].append(ent.text)\n",
    "for label, entities in entidades_dict.items():\n",
    "    print(f\"\\n{label}:\")\n",
    "    for ent in entities[:5]:\n",
    "        print(f\"  - {ent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"VERB\"}, {\"POS\": \"ADV\"}]\n",
    "matcher.add(\"Vigorous_Activities\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(f\"\\nPatrones Vigorous encontrados: {len(matches)}\")\n",
    "print(\"\\nPrimeros 5 matches:\")\n",
    "for ID, start, end in matches[:5]:\n",
    "    print(f\"  - {doc[start:end].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPalabras más frecuentes:\")\n",
    "palabra_freq = Counter()\n",
    "for token in doc:\n",
    "    if not token.is_stop and token.is_alpha:\n",
    "        palabra_freq[token.lemma_] += 1\n",
    "for palabra, freq in palabra_freq.most_common(20):\n",
    "    print(f\"{palabra:<20} {freq:>5} veces\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}