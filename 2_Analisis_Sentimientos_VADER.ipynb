{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Sentimientos con NLTK VADER\n",
    "## Análisis de reseñas de Amazon\n",
    "Análisis completo usando NLTK con VADER lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk scikit-learn matplotlib seaborn\n",
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"amazon_cells_labelled.txt\", sep=\"\\t\", header=None, names=[\"review\", \"sentiment\"])\n",
    "print(f\"Datos cargados: {len(df)} reseñas\")\n",
    "print(f\"\\nDistribución de sentimientos:\")\n",
    "print(df[\"sentiment\"].value_counts())\n",
    "print(f\"\\nPrimeras reseñas:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\\\S+|www\\\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\\\s.!?,-]\", \"\", text)\n",
    "    text = re.sub(r\"\\\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "df[\"review_cleaned\"] = df[\"review\"].apply(clean_text)\n",
    "df = df[df[\"review_cleaned\"].str.len() > 0]\n",
    "print(f\"Dataset después de limpieza: {len(df)} reseñas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = df[\"review_cleaned\"].apply(lambda x: sia.polarity_scores(x))\n",
    "sentiment_df = pd.DataFrame(sentiment_scores.tolist())\n",
    "df = pd.concat([df, sentiment_df], axis=1)\n",
    "df = df.rename(columns={\"neg\": \"vader_negative\", \"neu\": \"vader_neutral\", \"pos\": \"vader_positive\", \"compound\": \"vader_compound\"})\n",
    "print(\"VADER scores aplicados\")\n",
    "print(df[[\"review\", \"sentiment\", \"vader_compound\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vader_prediction\"] = df[\"vader_compound\"].apply(lambda x: 1 if x >= 0.05 else 0)\n",
    "print(\"Predicciones VADER creadas\")\n",
    "print(f\"Distribución de predicciones:\")\n",
    "print(df[\"vader_prediction\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "y_true = df[\"sentiment\"].values\n",
    "y_pred = df[\"vader_prediction\"].values\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "print(\"=\" * 60)\n",
    "print(\"MÉTRICAS DE CALIDAD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(f\"{\\\"\\\":<20} Predicción Neg  Predicción Pos\")\n",
    "print(f\"Real Negativo:      {cm[0,0]:>6}           {cm[0,1]:>6}\")\n",
    "print(f\"Real Positivo:      {cm[1,0]:>6}           {cm[1,1]:>6}\")\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negativo\", \"Positivo\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negativo\", \"Positivo\"], yticklabels=[\"Negativo\", \"Positivo\"])\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].hist(df[\"vader_compound\"], bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(x=0.05, color=\"green\", linestyle=\"--\", label=\"Threshold Pos\")\n",
    "axes[0].set_xlabel(\"Compound Score\")\n",
    "axes[0].set_title(\"Distribución de Scores\")\n",
    "axes[0].legend()\n",
    "df.boxplot(column=\"vader_compound\", by=\"sentiment\", ax=axes[1])\n",
    "axes[1].set_xlabel(\"Sentimiento Real\")\n",
    "axes[1].set_title(\"Compound por Sentimiento\")\n",
    "plt.suptitle(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de palabras influyentes\n",
    "positive_reviews = df[df[\"sentiment\"] == 1][\"review_cleaned\"].str.cat(sep=\" \")\n",
    "negative_reviews = df[df[\"sentiment\"] == 0][\"review_cleaned\"].str.cat(sep=\" \")\n",
    "positive_words = word_tokenize(positive_reviews)\n",
    "negative_words = word_tokenize(negative_reviews)\n",
    "positive_freq = Counter([w for w in positive_words if len(w) > 2])\n",
    "negative_freq = Counter([w for w in negative_words if len(w) > 2])\n",
    "print(\"Palabras en RESEÑAS POSITIVAS:\")\n",
    "for word, freq in positive_freq.most_common(10):\n",
    "    print(f\"  {word:<15} - {freq:>4} veces\")\n",
    "print(\"\\nPalabras en RESEÑAS NEGATIVAS:\")\n",
    "for word, freq in negative_freq.most_common(10):\n",
    "    print(f\"  {word:<15} - {freq:>4} veces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de casos mal clasificados\n",
    "df[\"correct\"] = df[\"sentiment\"] == df[\"vader_prediction\"]\n",
    "misclassified = df[~df[\"correct\"]]\n",
    "correctly_classified = df[df[\"correct\"]]\n",
    "fp = misclassified[(misclassified[\"sentiment\"] == 0) & (misclassified[\"vader_prediction\"] == 1)]\n",
    "fn = misclassified[(misclassified[\"sentiment\"] == 1) & (misclassified[\"vader_prediction\"] == 0)]\n",
    "print(\"ANÁLISIS DE ERRORES\")\n",
    "print(f\"Correctas: {len(correctly_classified)} ({len(correctly_classified)/len(df)*100:.2f}%)\")\n",
    "print(f\"Incorrectas: {len(misclassified)} ({len(misclassified)/len(df)*100:.2f}%)\")\n",
    "print(f\"Falsos Positivos: {len(fp)}\")\n",
    "print(f\"Falsos Negativos: {len(fn)}\")\n",
    "print(\"\\nEjemplos Falsos Positivos:\")\n",
    "for idx in fp.head(2).index:\n",
    "    print(f\"  {df.loc[idx, \\\"review\\\"][:60]}...\")\n",
    "print(\"\\nEjemplos Falsos Negativos:\")\n",
    "for idx in fn.head(2).index:\n",
    "    print(f\"  {df.loc[idx, \\\"review\\\"][:60]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}