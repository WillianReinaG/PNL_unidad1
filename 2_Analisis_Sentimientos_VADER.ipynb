{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Sentimientos con NLTK VADER\n",
    "## Análisis de reseñas de productos de Amazon\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WillianReinaG/PNL_unidad1/blob/main/2_Analisis_Sentimientos_VADER.ipynb)\n",
    "\n",
    "Este notebook realiza un análisis completo de sentimientos utilizando NLTK con VADER (Valence Aware Dictionary and sEntiment Reasoner)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción a VADER\n",
    "\n",
    "**VADER** es un lexicón y herramienta de análisis de sentimientos optimizada para redes sociales y textos cortos.\n",
    "\n",
    "Características principales:\n",
    "- Combina enfoques léxico y reglas basadas\n",
    "- Excelente para textos informales\n",
    "- Produce 4 scores: negativo, neutral, positivo y compound\n",
    "- El **compound score** (-1 a +1) resume el sentimiento total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instalación de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk scikit-learn pandas numpy matplotlib seaborn -q\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "print(\"✓ Dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "\n",
    "print(\"✓ Librerías importadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !wget https://raw.githubusercontent.com/WillianReinaG/PNL_unidad1/main/amazon_cells_labelled.txt -O ./amazon_cells_labelled.txt\n",
    "\n",
    "with open(\"./amazon_cells_labelled.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    df = pd.read_csv(file, sep=\"\\t\", header=None, names=[\"review\", \"sentiment\"])\n",
    "\n",
    "print(f\"Datos cargados: {len(df)} reseñas\")\n",
    "print(f\"Positivas: {(df['sentiment'] == 1).sum()} | Negativas: {(df['sentiment'] == 0).sum()}\")\n",
    "print(\"\\nPrimeras 5 reseñas:\") \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Información del dataset:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total de reseñas: {len(df)}\")\n",
    "print(f\"Caracteres promedio por reseña: {df['review'].str.len().mean():.0f}\")\n",
    "print(f\"Palabras promedio por reseña: {df['review'].str.split().str.len().mean():.1f}\")\n",
    "print(f\"\\nDistribución de sentimientos:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\nPorcentaje de cada clase:\")\n",
    "print(f\"Positivas: {(df['sentiment'] == 1).sum() / len(df) * 100:.2f}%\")\n",
    "print(f\"Negativas: {(df['sentiment'] == 0).sum() / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Limpia el texto para análisis\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.!?,-]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Limpiando textos...\")\n",
    "df['review_cleaned'] = df['review'].apply(clean_text)\n",
    "df = df[df['review_cleaned'].str.len() > 0]\n",
    "\n",
    "print(f\"Reseñas después de limpieza: {len(df)}\")\n",
    "print(\"\\nEjemplos de limpieza:\")\n",
    "for i in range(min(2, len(df))):\n",
    "    print(f\"Original: {df['review'].iloc[i][:60]}...\")\n",
    "    print(f\"Limpia:   {df['review_cleaned'].iloc[i][:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ejemplo simple: Análisis manual de una reseña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo_reseña = df['review_cleaned'].iloc[0]\n",
    "scores = sia.polarity_scores(ejemplo_reseña)\n",
    "\n",
    "print(\"ANÁLISIS DE UNA RESEÑA CON VADER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Reseña: {ejemplo_reseña}\")\n",
    "print(\"\\nScores VADER:\")\n",
    "print(f\"  Sentimiento Negativo: {scores['neg']:.4f}\")\n",
    "print(f\"  Sentimiento Neutral:  {scores['neu']:.4f}\")\n",
    "print(f\"  Sentimiento Positivo: {scores['pos']:.4f}\")\n",
    "print(f\"  Compound Score:       {scores['compound']:.4f}\")\n",
    "print(\"\\nInterpretación:\")\n",
    "if scores['compound'] >= 0.05:\n",
    "    print(\"  → SENTIMIENTO POSITIVO\")\n",
    "elif scores['compound'] <= -0.05:\n",
    "    print(\"  → SENTIMIENTO NEGATIVO\")\n",
    "else:\n",
    "    print(\"  → SENTIMIENTO NEUTRAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Análisis VADER en todo el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analizando {0} reseñas con VADER...\".format(len(df)))\n",
    "sentiment_scores = df['review_cleaned'].apply(lambda x: sia.polarity_scores(x))\n",
    "sentiment_df = pd.DataFrame(sentiment_scores.tolist())\n",
    "\n",
    "df = pd.concat([df, sentiment_df], axis=1)\n",
    "df = df.rename(columns={\n",
    "    \"neg\": \"vader_negative\",\n",
    "    \"neu\": \"vader_neutral\",\n",
    "    \"pos\": \"vader_positive\",\n",
    "    \"compound\": \"vader_compound\"\n",
    "})\n",
    "\n",
    "print(\"✓ Análisis completado\")\n",
    "print(\"\\nEstadísticas de Compound Scores:\")\n",
    "print(df['vader_compound'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convertir scores a predicciones binarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vader_prediction'] = df['vader_compound'].apply(lambda x: 1 if x >= 0.05 else 0)\n",
    "\n",
    "print(\"Predicciones generadas\")\n",
    "print(\"\\nDistribución de predicciones:\")\n",
    "print(df['vader_prediction'].value_counts())\n",
    "print(\"\\nComparación predicciones vs reales:\")\n",
    "comparison = pd.crosstab(df['sentiment'], df['vader_prediction'], \n",
    "                         rownames=[\"Real\"], colnames=[\"Predicción\"])\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cálculo de métricas de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df['sentiment'].values\n",
    "y_pred = df['vader_prediction'].values\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MÉTRICAS DE EVALUACIÓN\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} (de positivas predichas, cuántas eran correctas)\")\n",
    "print(f\"Recall:    {recall:.4f} (de positivas reales, cuántas detectó)\")\n",
    "print(f\"F1-Score:  {f1:.4f} (media armónica de precision y recall)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Matriz de confusión y reporte de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"{0:<20} {1:<20} {2:<20}\".format(\"\", \"Pred. Negativo\", \"Pred. Positivo\"))\n",
    "print(\"{0:<20} {1:<20} {2:<20}\".format(\"Real Negativo\", str(cm[0,0]), str(cm[0,1])))\n",
    "print(\"{0:<20} {1:<20} {2:<20}\".format(\"Real Positivo\", str(cm[1,0]), str(cm[1,1])))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Negativo', 'Positivo']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualización 1: Matriz de Confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Negativo\", \"Positivo\"],\n",
    "            yticklabels=[\"Negativo\", \"Positivo\"])\n",
    "plt.title(\"Matriz de Confusión - VADER Sentiment Analysis\")\n",
    "plt.ylabel(\"Sentimiento Real\")\n",
    "plt.xlabel(\"Sentimiento Predicho\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualización 2: Distribución de Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['vader_compound'], bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(x=0.05, color=\"green\", linestyle=\"--\", label=\"Threshold Positivo\")\n",
    "axes[0].axvline(x=-0.05, color=\"red\", linestyle=\"--\", label=\"Threshold Negativo\")\n",
    "axes[0].set_xlabel(\"Compound Score\")\n",
    "axes[0].set_title(\"Distribución de Compound Scores\")\n",
    "axes[0].legend()\n",
    "\n",
    "df.boxplot(column=\"vader_compound\", by=\"sentiment\", ax=axes[1])\n",
    "axes[1].set_xlabel(\"Sentimiento Real (0=Neg, 1=Pos)\")\n",
    "axes[1].set_ylabel(\"Compound Score\")\n",
    "axes[1].set_title(\"Compound Por Sentimiento\")\n",
    "plt.suptitle(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Análisis de componentes VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(df['vader_negative'], bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "axes[0].set_title(\"Distribución de Negatividad\")\n",
    "axes[0].set_xlabel(\"Negative Score\")\n",
    "\n",
    "axes[1].hist(df['vader_neutral'], bins=30, alpha=0.7, edgecolor=\"black\", color=\"orange\")\n",
    "axes[1].set_title(\"Distribución de Neutralidad\")\n",
    "axes[1].set_xlabel(\"Neutral Score\")\n",
    "\n",
    "axes[2].hist(df['vader_positive'], bins=30, alpha=0.7, edgecolor=\"black\", color=\"green\")\n",
    "axes[2].set_title(\"Distribución de Positividad\")\n",
    "axes[2].set_xlabel(\"Positive Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Análisis de palabras más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = df[df['sentiment'] == 1]['review_cleaned'].str.cat(sep=' ')\n",
    "negative_reviews = df[df['sentiment'] == 0]['review_cleaned'].str.cat(sep=' ')\n",
    "\n",
    "positive_words = word_tokenize(positive_reviews)\n",
    "negative_words = word_tokenize(negative_reviews)\n",
    "\n",
    "positive_freq = Counter([w for w in positive_words if len(w) > 2 and w.isalpha()])\n",
    "negative_freq = Counter([w for w in negative_words if len(w) > 2 and w.isalpha()])\n",
    "\n",
    "print(\"Palabras más frecuentes en reseñas POSITIVAS:\")\n",
    "print(\"-\" * 50)\n",
    "for word, freq in positive_freq.most_common(15):\n",
    "    print(\"{0:<15} {1:>5} veces\".format(word, freq))\n",
    "\n",
    "print(\"\\n\\nPalabras más frecuentes en reseñas NEGATIVAS:\")\n",
    "print(\"-\" * 50)\n",
    "for word, freq in negative_freq.most_common(15):\n",
    "    print(\"{0:<15} {1:>5} veces\".format(word, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Análisis de errores (casos mal clasificados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['correct'] = df['sentiment'] == df['vader_prediction']\n",
    "misclassified = df[~df['correct']]\n",
    "\n",
    "fp = misclassified[(misclassified['sentiment'] == 0) & (misclassified['vader_prediction'] == 1)]\n",
    "fn = misclassified[(misclassified['sentiment'] == 1) & (misclassified['vader_prediction'] == 0)]\n",
    "\n",
    "print(\"ANÁLISIS DE ERRORES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Correctas: {(df['correct']).sum()} ({(df['correct']).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"Incorrectas: {len(misclassified)} ({len(misclassified)/len(df)*100:.2f}%)\")\n",
    "print(f\"  - Falsos Positivos (real negativo, predicción positiva): {len(fp)}\")\n",
    "print(f\"  - Falsos Negativos (real positivo, predicción negativa): {len(fn)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EJEMPLOS DE FALSOS POSITIVOS:\")\n",
    "print(\"=\" * 70)\n",
    "for idx in fp.head(3).index:\n",
    "    print(f\"Reseña: {df.loc[idx, 'review'][:70]}...\")\n",
    "    print(f\"Score: {df.loc[idx, 'vader_compound']:.3f} (predicción: positivo, real: negativo)\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EJEMPLOS DE FALSOS NEGATIVOS:\")\n",
    "print(\"=\" * 70)\n",
    "for idx in fn.head(3).index:\n",
    "    print(f\"Reseña: {df.loc[idx, 'review'][:70]}...\")\n",
    "    print(f\"Score: {df.loc[idx, 'vader_compound']:.3f} (predicción: negativo, real: positivo)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Resumen y conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESUMEN FINAL DEL ANÁLISIS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset: {len(df)} reseñas\")\n",
    "print(f\"Modelo: VADER (Valence Aware Dictionary and sEntiment Reasoner)\")\n",
    "print(f\"\\nRendimiento:\")\n",
    "print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"\\nVentajas de VADER:\")\n",
    "print(\"  ✓ Excelente para textos cortos (redes sociales)\")\n",
    "print(\"  ✓ Maneja emojis y puntuación\")\n",
    "print(\"  ✓ Rápido y sin necesidad de entrenamiento\")\n",
    "print(\"  ✓ Interpretable\")\n",
    "print(f\"\\nLimitaciones:\")\n",
    "print(\"  ✗ Puede no captar sarcasmo bien\")\n",
    "print(\"  ✗ Sesgo hacia vocabulario común\")\n",
    "print(\"  ✗ Peor desempeño con textos largos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}