import json

# Notebook 1 mejorado: spaCy NLP
notebook1 = {
    'cells': [
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '# Procesamiento de Lenguaje Natural con spaCy\n',
                '## An√°lisis del texto: "El Coraz√≥n Delator" de Edgar Allan Poe\n',
                '\n',
                '[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WillianReinaG/PNL_unidad1/blob/main/1_NLP_spacy_ElCorazonDelator.ipynb)\n',
                '\n',
                'Este notebook realiza un **an√°lisis completo de Procesamiento de Lenguaje Natural (NLP)** en espa√±ol utilizando **spaCy**.\n',
                '\n',
                '**Contenidos:**\n',
                '- Tokenizaci√≥n y an√°lisis de tokens\n',
                '- Etiquetado de partes del lenguaje (POS tagging)\n',
                '- An√°lisis de dependencias (Dependency parsing)\n',
                '- Extracci√≥n de sintagmas nominales\n',
                '- Identificaci√≥n de verbos\n',
                '- Reconocimiento de entidades nombradas (NER)\n',
                '- B√∫squeda de patrones con Matcher\n',
                '- An√°lisis de frecuencia de palabras'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üì¶ Paso 1: Instalaci√≥n de dependencias\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Instala spaCy (librer√≠a de NLP) y descarga el modelo pre-entrenado para espa√±ol `es_core_news_sm`. Este modelo incluye:\n',
                '- Tokenizador\n',
                '- Etiquetador de POS (Part-of-Speech)\n',
                '- Analizador de dependencias\n',
                '- Reconocedor de entidades\n',
                '\n',
                '**Nota:** En Google Colab, esto puede tomar 2-3 minutos.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                '!pip install spacy -q\n',
                '!python -m spacy download es_core_news_sm -q\n',
                'print("‚úì spaCy instalado correctamente")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üìö Paso 2: Importar librer√≠as necesarias\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Carga las librer√≠as que utilizaremos:\n',
                '- **spacy**: Para procesamiento NLP\n',
                '- **Matcher**: Para b√∫squeda de patrones en texto\n',
                '- **pandas**: Para manipulaci√≥n de datos\n',
                '- **Counter**: Para contar frecuencias\n',
                '\n',
                'Tambi√©n carga el modelo de espa√±ol que descargamos.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'import spacy\n',
                'from spacy.matcher import Matcher\n',
                'import pandas as pd\n',
                'from collections import Counter\n',
                '\n',
                '# Cargar el modelo de espa√±ol\n',
                'nlp = spacy.load("es_core_news_sm")\n',
                'print("‚úì Modelo de espa√±ol cargado")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üìÑ Paso 3: Cargar el texto a analizar\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Lee el archivo de texto "El Coraz√≥n Delator.txt" que contiene el relato completo.\n',
                'En Google Colab, puedes subir archivos locales o conectarte a Google Drive.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'with open("El_coraz√≥n_delator.txt", "r", encoding="utf-8") as f:\n',
                '    texto = f.read()\n',
                '\n',
                'print(f"Texto cargado correctamente")\n',
                'print(f"Longitud: {len(texto)} caracteres")\n',
                'print(f"\\nPrimer p√°rrafo (primeros 200 caracteres):")\n',
                'print(texto[:200] + "...")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## ‚öôÔ∏è Paso 4: Procesar el texto con spaCy\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Aplica el pipeline de spaCy al texto. Esto significa:\n',
                '1. **Tokenizaci√≥n**: Divide el texto en tokens (palabras, puntuaci√≥n, etc.)\n',
                '2. **An√°lisis morfol√≥gico**: Analiza partes del lenguaje y lemas\n',
                '3. **An√°lisis sint√°ctico**: Detecta dependencias entre palabras\n',
                '4. **NER**: Identifica entidades nombradas (personas, lugares, etc.)\n',
                '\n',
                '‚è±Ô∏è Esto puede tomar algunos segundos.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                '# Procesar el texto\n',
                'doc = nlp(texto)\n',
                'print("‚úì Texto procesado con spaCy")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üî¢ Paso 5: An√°lisis de TOKENS\n',
                '\n',
                '### ¬øQu√© es un token?\n',
                'Un token es la unidad m√°s peque√±a de an√°lisis: una palabra, n√∫mero, signo de puntuaci√≥n, etc.\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Cuenta el n√∫mero total de tokens en el documento.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'total_tokens = len(doc)\n',
                'print(f"üìä Total de tokens en el archivo: {total_tokens}")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üìù Paso 6: An√°lisis de ORACIONES\n',
                '\n',
                '### ¬øQu√© es una oraci√≥n?\n',
                'Una oraci√≥n es una secuencia de tokens que forma una unidad gramatical completa.\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Cuenta el n√∫mero total de oraciones y muestra las primeras 3 como ejemplo.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'sentences = list(doc.sents)\n',
                'total_sentences = len(sentences)\n',
                '\n',
                'print(f"üìä Total de oraciones: {total_sentences}")\n',
                'print(f"\\nPrimeras 3 oraciones:")\n',
                'for i, sent in enumerate(sentences[:3], 1):\n',
                '    print(f"{i}. {sent.text[:100]}...")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üéØ Paso 7: Extracci√≥n de la TERCERA ORACI√ìN\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Extrae y muestra la tercera oraci√≥n del documento completo.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'if len(sentences) >= 3:\n',
                '    tercera_oracion = sentences[2]\n',
                '    print("üìå Tercera oraci√≥n del documento:")\n',
                '    print(f"\\n{tercera_oracion.text}")\n',
                'else:\n',
                '    print(f"El documento tiene solo {len(sentences)} oraciones")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üîç Paso 8: An√°lisis DETALLADO de tokens de la tercera oraci√≥n\n',
                '\n',
                '### ¬øQu√© son POS tags y DEP tags?\n',
                '- **POS tag (Part-Of-Speech)**: La categor√≠a gramatical (NOUN=sustantivo, VERB=verbo, ADJ=adjetivo, etc.)\n',
                '- **DEP tag (Dependency tag)**: La relaci√≥n sint√°ctica de la palabra en la oraci√≥n (sujeto, verbo, objeto, etc.)\n',
                '- **LEMMA**: La forma can√≥nica de la palabra (ej: "corriendo" ‚Üí "correr")\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Para cada token de la tercera oraci√≥n, muestra:\n',
                '1. El texto del token\n',
                '2. Su POS tag\n',
                '3. Su DEP tag\n',
                '4. Su lema'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'if len(sentences) >= 3:\n',
                '    tercera_oracion = sentences[2]\n',
                '    print("An√°lisis detallado de tokens:")\n',
                '    print("-" * 80)\n',
                '    print(f"{\\"Token\\":<15} {\\"POS Tag\\":<12} {\\"DEP Tag\\":<12} {\\"Lemma\\":<15}")\n',
                '    print("-" * 80)\n',
                '    for token in tercera_oracion:\n',
                '        print(f"{token.text:<15} {token.pos_:<12} {token.dep_:<12} {token.lemma_:<15}")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üè∑Ô∏è Paso 9: Extracci√≥n de SINTAGMAS NOMINALES (Noun Chunks)\n',
                '\n',
                '### ¬øQu√© es un sintagma nominal?\n',
                'Un sintagma nominal es un grupo de palabras que funciona como sustantivo.\n',
                'Ejemplo: "El viejo coraz√≥n rojo" es un sintagma nominal.\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Extrae todos los sintagmas nominales del documento y muestra los primeros 15.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'noun_chunks = list(doc.noun_chunks)\n',
                'print(f"üìä Total de sintagmas nominales: {len(noun_chunks)}")\n',
                'print(f"\\nPrimeros 15 sintagmas nominales:")\n',
                'for i, chunk in enumerate(noun_chunks[:15], 1):\n',
                '    print(f"{i}. {chunk.text}")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üî§ Paso 10: Extracci√≥n de VERBOS\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Busca todos los verbos (tokens con POS tag = VERB) en el documento.\n',
                'Muestra los verbos √∫nicos (por su lema) para evitar duplicados como "habla", "hablaba", "hablaban".'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'verbos = [token for token in doc if token.pos_ == "VERB"]\n',
                'verbos_unicos = sorted(set([v.lemma_ for v in verbos]))\n',
                '\n',
                'print(f"üìä Total de verbos: {len(verbos)}")\n',
                'print(f"Verbos √∫nicos: {len(verbos_unicos)}")\n',
                'print(f"\\nPrimeros 20 verbos encontrados:")\n',
                'for i, verbo in enumerate(verbos_unicos[:20], 1):\n',
                '    print(f"{i}. {verbo}")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üè¢ Paso 11: RECONOCIMIENTO DE ENTIDADES NOMBRADAS (NER)\n',
                '\n',
                '### ¬øQu√© es una entidad nombrada?\n',
                'Una entidad nombrada es un nombre espec√≠fico de:\n',
                '- **PERSON**: Personas\n',
                '- **ORG**: Organizaciones\n',
                '- **GPE**: Lugares geogr√°ficos/pol√≠ticos\n',
                '- **DATE**: Fechas\n',
                '- Y otras categor√≠as...\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Identifica todas las entidades nombradas en el texto y las agrupa por tipo.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'print("üè∑Ô∏è Entidades Nombradas encontradas:")\n',
                'entidades_dict = {}\n',
                'for ent in doc.ents:\n',
                '    if ent.label_ not in entidades_dict:\n',
                '        entidades_dict[ent.label_] = []\n',
                '    if ent.text not in entidades_dict[ent.label_]:\n',
                '        entidades_dict[ent.label_].append(ent.text)\n',
                '\n',
                'for label, entities in sorted(entidades_dict.items()):\n',
                '    print(f"\\n{label}:")\n',
                '    for ent in entities[:5]:\n',
                '        print(f"  - {ent}")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üîé Paso 12: B√öSQUEDA DE PATRONES con Matcher\n',
                '\n',
                '### ¬øQu√© es un Matcher?\n',
                'El Matcher es una herramienta que busca patrones espec√≠ficos de tokens en el texto.\n',
                'Por ejemplo, puedes buscar: "Verbo seguido de Adverbio" para encontrar acciones con adverbios modificadores.\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Busca todos los patrones donde un VERBO es seguido por un ADVERBIO (actividades con intensidad).\n',
                'Ejemplo: "habla r√°pidamente", "act√∫a cautelosamente", etc.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'matcher = Matcher(nlp.vocab)\n',
                'pattern = [{\"POS\": \"VERB\"}, {\"POS\": \"ADV\"}]\n',
                'matcher.add("Vigorous_Activities", [pattern])\n',
                'matches = matcher(doc)\n',
                '\n',
                'print(f"üîé Patrones encontrados (VERBO + ADVERBIO): {len(matches)}")\n',
                'print(f"\\nPrimeros 5 matches:")\n',
                'for i, (match_id, start, end) in enumerate(matches[:5], 1):\n',
                '    print(f"{i}. {doc[start:end].text}")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üìä Paso 13: ESTAD√çSTICAS DE PALABRAS\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Identifica las palabras m√°s frecuentes en el documento.\n',
                'Excluye "stop words" (palabras comunes como "el", "la", "de", etc.) para mostrar palabras significativas.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'palabra_freq = Counter()\n',
                'for token in doc:\n',
                '    if not token.is_stop and token.is_alpha:\n',
                '        palabra_freq[token.lemma_] += 1\n',
                '\n',
                'print("üìä 20 palabras m√°s frecuentes (sin stop words):")\n',
                'print("-" * 40)\n',
                'for palabra, freq in palabra_freq.most_common(20):\n',
                '    print(f"{palabra:<20} {freq:>5} veces")'
            ]
        },
        {
            'cell_type': 'markdown',
            'metadata': {},
            'source': [
                '## üìà Paso 14: RESUMEN EJECUTIVO\n',
                '\n',
                '### ¬øQu√© hace esta celda?\n',
                'Muestra un resumen de todos los an√°lisis realizados en un formato f√°cil de leer.'
            ]
        },
        {
            'cell_type': 'code',
            'execution_count': None,
            'metadata': {},
            'outputs': [],
            'source': [
                'print("=" * 80)\n',
                'print("RESUMEN EJECUTIVO DEL AN√ÅLISIS NLP")\n',
                'print("=" * 80)\n',
                'print(f"\\nüìä ESTAD√çSTICAS GENERALES:")\n',
                'print(f"   ‚Ä¢ Total de tokens: {total_tokens}")\n',
                'print(f"   ‚Ä¢ Total de oraciones: {total_sentences}")\n',
                'print(f"   ‚Ä¢ Sintagmas nominales: {len(noun_chunks)}")\n',
                'print(f"   ‚Ä¢ Verbos: {len(verbos)}")\n',
                'print(f"   ‚Ä¢ Verbos √∫nicos: {len(verbos_unicos)}")\n',
                'print(f"   ‚Ä¢ Entidades nombradas: {len(doc.ents)}")\n',
                'print(f"   ‚Ä¢ Patrones VERBO+ADVERBIO: {len(matches)}")\n',
                'print(f"\\nüìù LONGITUD PROMEDIO:")\n',
                'print(f"   ‚Ä¢ Tokens por oraci√≥n: {total_tokens / total_sentences:.2f}")\n',
                'print(f"   ‚Ä¢ Caracteres: {len(texto)}")\n',
                'print("\\n‚úì An√°lisis completado exitosamente")'
            ]
        }
    ],
    'metadata': {
        'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'},
        'language_info': {'name': 'python', 'version': '3.8.0'}
    },
    'nbformat': 4,
    'nbformat_minor': 4
}

# Guardar notebook
import os
os.chdir(r'C:\Users\bebes\Documents\MIAA\3. SEMESTRE\2. NLP TRANSFORMES\icesi_NLP_unidad1')

with open('1_NLP_spacy_ElCorazonDelator.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook1, f, ensure_ascii=False, indent=1)

print('‚úì Notebook 1 mejorado y guardado')
